{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mND5QIXH2VZi",
        "outputId": "9506bd9b-6733-486f-f7eb-1e9849f24c82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Get:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:8 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Fetched 252 kB in 5s (52.3 kB/s)\n",
            "Reading package lists... Done\n",
            "--2022-03-01 00:49:09--  https://jdbc.postgresql.org/download/postgresql-42.2.9.jar\n",
            "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
            "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 914037 (893K) [application/java-archive]\n",
            "Saving to: ‘postgresql-42.2.9.jar.1’\n",
            "\n",
            "postgresql-42.2.9.j 100%[===================>] 892.61K  4.51MB/s    in 0.2s    \n",
            "\n",
            "2022-03-01 00:49:09 (4.51 MB/s) - ‘postgresql-42.2.9.jar.1’ saved [914037/914037]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# for example:\n",
        "# spark_version = 'spark-3.0.3'\n",
        "spark_version = 'spark-3.0.3'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# start a SparkSession\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"CloudETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.9.jar\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cS8dvVVo2ZZZ"
      },
      "outputs": [],
      "source": [
        "# configure settings and authentication for RDS\n",
        "mode=\"append\"\n",
        "jdbc_url=\"<end-point>\"\n",
        "config = {\"user\":\"<username>\", \n",
        "          \"password\": \"<password>\", \n",
        "          \"driver\":\"org.postgresql.Driver\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSDddBmT2cHY"
      },
      "outputs": [],
      "source": [
        "# url to the s3 bucket with csv\n",
        "url1=\"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Office_Products_v1_00.tsv.gz\"\n",
        "csv1=\"amazon_reviews_us_Office_Products_v1_00.tsv.gz\"\n",
        "url2=\"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Lawn_and_Garden_v1_00.tsv.gz\"\n",
        "csv2=\"amazon_reviews_us_Lawn_and_Garden_v1_00.tsv.gz\"\n",
        "url3=\"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Home_Improvement_v1_00.tsv.gz\"\n",
        "csv3=\"amazon_reviews_us_Home_Improvement_v1_00.tsv.gz\"\n",
        "\n",
        "# lists of url and csv\n",
        "url_list = [url1, url2, url3]\n",
        "csv_list = [csv1, csv2, csv3]\n",
        "\n",
        "from pyspark import SparkFiles\n",
        "from pyspark.sql.functions import col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3-HGHO72hi3",
        "outputId": "96fe6832-7927-47ce-e632-50e74f760932"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 0\n",
            "Iteration: 1\n",
            "Iteration: 2\n"
          ]
        }
      ],
      "source": [
        "# looping through url_list length\n",
        "for num in range(0,len(url_list)):\n",
        "\n",
        "  # reading csv into df\n",
        "  print('Iteration:', num)\n",
        "  spark.sparkContext.addFile(url_list[num])\n",
        "  df = spark.read.csv(SparkFiles.get(csv_list[num]), sep=\"\\t\", header=True, inferSchema=True)\n",
        "\n",
        "  # dropping duplicates and dropping nulls\n",
        "  df = df.dropDuplicates()\n",
        "  df = df.na.drop(\"all\")\n",
        "\n",
        "  # creating tables\n",
        "  # creating review_id_table\n",
        "  review_id_table = df.select([\"review_id\", \"customer_id\", \"product_id\", \"product_parent\", \"product_category\", \"review_date\"])\n",
        "  # changing customer_id datatype to int, product_parent datatype to int, and review_date to datetype\n",
        "  from pyspark.sql.types import IntegerType,BooleanType,DateType\n",
        "  review_id_table = review_id_table.withColumn(\"customer_id\", review_id_table.customer_id.cast(\"int\")).withColumn(\"product_parent\", review_id_table.product_parent.cast(\"int\")).withColumn(\"review_date\", review_id_table.review_date.cast(DateType()))\n",
        "  review_id_table = review_id_table.dropDuplicates([\"review_id\"])\n",
        "  \n",
        "  # creating product table\n",
        "  products = df.select([\"product_id\", \"product_title\"])\n",
        "  products = products.dropDuplicates([\"product_id\"])\n",
        "  products = products.na.drop(\"all\")\n",
        "  # reading old table and appending new table and removing duplicates and nulls\n",
        "  old_products = spark.read.jdbc(url=jdbc_url, table='products', properties=config)\n",
        "  final_products = products.union(old_products).dropDuplicates().na.drop(\"all\")\n",
        "\n",
        "  # creating customers table\n",
        "  customers = df.select([\"customer_id\"])\n",
        "  customers = customers.withColumn(\"customer_id\", customers.customer_id.cast(\"int\"))\n",
        "  customers = customers.groupBy(\"customer_id\").count()\n",
        "  customers = customers.withColumnRenamed(\"count\", \"customer_count\").withColumn(\"customer_count\", customers.customer_id.cast(\"int\"))\n",
        "  # reading old table and appending new table and recounting customer_id\n",
        "  customersdf = spark.read.jdbc(url=jdbc_url, table='customers', properties=config)\n",
        "  final_customers = customersdf.union(customers)\n",
        "  final_customers = final_customers.groupBy(\"customer_id\").sum('customer_count').withColumnRenamed(\"sum(customer_count)\", \"customer_count\").na.drop(\"all\")\n",
        "\n",
        "  # creating vine table\n",
        "  vine_table = df.select([\"review_id\", \"star_rating\", \"helpful_votes\", \"total_votes\", \"vine\"])\n",
        "  # casting star_rating to int\n",
        "  vine_table = vine_table.withColumn(\"star_rating\", vine_table.star_rating.cast(\"int\")).withColumn(\"helpful_votes\", vine_table.helpful_votes.cast(\"int\")).withColumn(\"total_votes\",vine_table.total_votes.cast(\"int\"))\n",
        "  vine_table = vine_table.dropDuplicates([\"review_id\"])\n",
        "  vine_table = vine_table.na.drop(\"all\")\n",
        "\n",
        "  # appending to tables in rds database\n",
        "  review_id_table.write.jdbc(url=jdbc_url, table='review_id_table', mode=mode, properties=config)\n",
        "  final_products.write.jdbc(url=jdbc_url, table='products', mode=\"overwrite\", properties=config)\n",
        "  final_customers.write.jdbc(url=jdbc_url, table='customers', mode=\"overwrite\", properties=config)\n",
        "  vine_table.write.jdbc(url=jdbc_url, table='vine_table', mode=mode, properties=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5hwwe_cY1ZW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ETL.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
